
-- USER DEFINED FUNCTIONS
-- 1) Custom functions for run-time manipulation. 2) 

# add udf in hive shell
add file /tmp/hive_class/multiply_udf.py;

import sys
for line in sys.stdin:
	line = line.strip("\n\r")
    quantity = int(line)
    quantity = quantity*10
    print(str(quantity))

# statement to execute python udf
select transform(year_id,quantityordered) using 'python multicol_python_udf.py' as (year_id string, square int) from sales_order_data_orc
 limit 5;


# add udf in hive shell

add file /tmp/hive_class/many_column_udf.py;

import sys
                                                                                                                                              
for line in sys.stdin:
        line = line.strip("\n\r")
        country, order_num, quantity = line.split("\t")
        order_num = int(order_num)
        quantity = int(quantity)
        quantity = quantity*1000
        result = '\t'.join([country,str(order_num),str(quantity)])
        print(result)

import sys
for line in sys.stdin:
	line = line.strip('/n/r')
    country, order_num, quantity = line.split('\n')
    quantity = int(quantity)
    quantity =quantity*1000
    order_num = int(order_num)
    result = '/t'.join([country, str(order_num), str(quantity)])
    print(result)

# statement to execute python udf for multiple columns

hive> select transform(country,ordernumber,quantityordered)                                                                                   
    > using 'python many_column_udf.py' as (country string, ordernumber int, multiplied_quantity int)                                         
    > from sales_order_data_orc limit 10;
    
    
# use below hive statements for bucketing

hive> create table users                                                                                                                      
    > (                                                                                                                                       
    > id int,                                                                                                                                 
    > name string,                                                                                                                            
    > salary int,                                                                                                                             
    > unit string                                                                                                                             
    > )row format delimited                                                                                                                   
    > fields terminated by ','; 
  
load data local inpath 'file:///tmp/hive_class/users.csv' into table users;
    
hive> create table locations                                                                                                                  
    > (                                                                                                                                       
    > id int,                                                                                                                                 
    > location string                                                                                                                         
    > )                                                                                                                                       
    > row format delimited                                                                                                                    
    > fields terminated by ','; 
    
load data local inpath 'file:///tmp/hive_class/locations.csv' into table locations; 

set hive.enforce.bucketing=true;
    
    
 hive> create table buck_users                                                                                                                 
    > (                                                                                                                                       
    > id int,                                                                                                                                 
    > name string,                                                                                                                            
    > salary int,                                                                                                                             
    > unit string                                                                                                                             
    > )                                                                                                                                       
    > clustered by (id)                                                                                                                       
    > sorted by (id)                                                                                                                          
    > into 2 buckets;
    
insert overwrite table buck_users select * from users;
    
hive> create table buck_locations                                                                                                             
    > (                                                                                                                                       
    > id int,                                                                                                                                 
    > location string                                                                                                                         
    > )                                                                                                                                       
    > clustered by (id)                                                                                                                       
    > sorted by (id)                                                                                                                          
    > into 2 buckets; 
    
 insert overwrite table buck_locations select * from locations;
 
 
 
 
 ------------------------------------------------------------------------------------------------------------------------------
Reduce-Side Join
------------------------------------------------------------------------------------------------------------------------------

SET hive.auto.convert.join=false;
SELECT * FROM buck_users u INNER JOIN buck_locations l ON u.id = l.id;

Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1

------------------------------------------------------------------------------------------------------------------------------
Map Side Join
------------------------------------------------------------------------------------------------------------------------------

SET hive.auto.convert.join=true;
SELECT * FROM buck_users u INNER JOIN buck_locations l ON u.id = l.id;

Mapred Local Task Succeeded . Convert the Join into MapJoin
Number of reduce tasks is set to 0 since there is no reduce operator
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 0

------------------------------------------------------------------------------------------------------------------------------
Bucket Map Join
------------------------------------------------------------------------------------------------------------------------------
set hive.optimize.bucketmapjoin=true;
SET hive.auto.convert.join=true;

SELECT * FROM buck_users u INNER JOIN buck_locations l ON u.id = l.id;

------------------------------------------------------------------------------------------------------------------------------
Sorted Merge Bucket Map Join
------------------------------------------------------------------------------------------------------------------------------
set hive.enforce.sortmergebucketmapjoin=false;
set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;


SET hive.auto.convert.join=false;
SELECT * FROM buck_users u INNER JOIN buck_locations l ON u.id = l.id;

No MapLocal Task to create hash table.
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 0
